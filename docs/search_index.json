[["index.html", "March Madness Machine Learning 2022 Chapter 1 Overview", " March Madness Machine Learning 2022 Connor Folk 2022-03-05 Chapter 1 Overview March is here and everyone is talking about the NCAA tournament. Being a UNC fan, the talk this year is whether or not we are going (or should) to make the tournament. After watching ESPN talk about the seeding and teams, I wondered if I could use my data science knowledge to predict the outcome of these games. I am not talking about a perfect bracket, but one that used team stats to predict games. Could this model beat other peoples brackets and maybe win me a bit of money. As I initially searched the internet for NCAA basketball data, I stumbled upon a Kaggle competition that happens each year. This competition contained all the data I needed (and much more) to train some machine learning models. If you want to check out the site of this competition, it is listed here: https://www.kaggle.com/c/mens-march-mania-2022/ "],["dataset.html", "Chapter 2 Dataset", " Chapter 2 Dataset The data contains 20 CSV files with team names, matchups, box score stats, and rankings dating back to 2003. I started by looking at the box scores of each team since these statistics would be the basis of my models. Each row had a Winning Team and Losing Team along with their respective stats. I pivoted this dataset longer in order to have a row for each team in a game (1 game has 2 associated rows, one for each team). These rows contained offensive statistics for the team like points and defensive statistics like points allowed. Then, I performed feature creation on the game stats: adding offensive rebounds and defensive rebounds together to calculate total rebound, subtracting points scored and allowed for point differential, and calculating possessions from shot attempts. After creating these columns, I rolled the game data into season data for each team by calculating the averages of each statistic over a season. I also created a few other features including Points per Possession and effective Field Goal Percentage. After calculating the season statistics for each team, I joined the dataset with the ranking CSV. This file had rankings for each team, including the AP Poll, NET ranking, and Pomeroy ranking. I also created a sort of heat index, which showed the change in a teams Pom ranking over the last month before the tournament. I joined this table with seasonal and ranking data for each team with the the file of team matchups for the 2003 to 2021 tournaments. Each row, now, contained Team 1 and Team 2 and their respective statistics for the year. The results were set up as a binary indicator coinciding with a 1 if Team1 won and a 0 if Team 2 won. This means the probability of Team 1 winning is just p, while the probability of Team 2 winning is 1-p.Â The dataset is also evenly split between games that Team 1 and Team 2 won. Lastly, I filtered the training data to only include data from the 2008 to 2018 seasons. This ensured that the model had enough data point (around 600) to find patterns, but did not look back too far at seasons in which the landscape of college basketball was vastly different. My validation and test sets were the 2019 and 2021 seasons and tournaments (NOTE: There was no tournament in 2020 due to COVID-19) "],["variable-selection.html", "Chapter 3 Variable Selection", " Chapter 3 Variable Selection The dataset contained 112 different variables or metrics for each team. Using all of these variables provided the modeling techniques too much noise. Many of the modeling techniques predicted the training data very well with all of the variables, but performed poorly on the test and validation. Using a combination of variable importance plots, modeling assesment metrics, and knowledge of college basketball, I choose the following variables: Points per Possession (Offensive Efficiency)- Average number of points a team scores per possession Opponents Points per Possession (Defensive Efficiency)- Average number of points allowed per possession Point Differential- Margin of victory, number of points scored minus number of points allowed Pomeroy Ranking- Ranking of teams by legendary college basketball statistician Ken Pom (lower the ranking, the better) Opponent Three Point Field Goal Percentage- Number of three points allowed divided by number of three points attempted Free Throw Percentage- Number of free throws made/ number of free throws attempted Offensive Rebound Difference- Difference between a teams number of offensive rebound and their opponents number of offensive rebounds Opponent Turnovers- Number of turnovers by a teams opponent These variables account for a teams defensive, offensive, rebounding, and overall abilities. "],["models.html", "Chapter 4 Models 4.1 XGBoost 4.2 Random Forest Model 4.3 Neural Network 4.4 Ensemble Models", " Chapter 4 Models After data cleaning and variable selection, I created different models, including logistic regression, xgboost, random forest, and neural networks and scored them based on area under the ROC curve (AROC) and misclassification rate. AROC showed the models performance across different cutoffs, which may not seem useful since the cutoff should be .5. However, this metric allowed me to asses the overall performance of the model, not just classification. The three best models were an xgboost model, a random forest model, and a neural net model. 4.1 XGBoost 4.1.1 Methodology and Analysis In order to create this model, I had to turn the parameters to the dataset. I used 30-fold cross validation to pick the parameters that resulted in the lowest amount of error and prevented overfitting. However, this model still overfit the training data. The ROC curve and Confusion Matrix for this model on the training data is shown below. The model does extradinarily well with a AROC of .94 and misclassification rate of -. Predicted Team 2 Won Predicted Team 1 Won Actual Team 2 Won 322 43 Actual Team 1 Won 42 321 Classification Rate 88.32% The model also calculates the importance of each variable in predicting which team wins. The variables are clustered based on importance to the model, with cluster 1 in red and cluster 2 in light blue. The two most important variables were the Pomeroy rankings for the two teams. The plot of Variable Importance is shown below. 4.1.2 Results Even though the model performed extremely well on the training set, it does not perform as well on the validation dataset (2019 and 2021 seasons) compared to the training dataset furthering the evidence that the model is overfitted However, the model performs very well on the 2019 NCAA tournament compared to other models I created, predicting about 76% of the games correctly. However, the model performs extremely poorly on the 2021 NCAA tournament, predicting only 63% of games correctly. ____ Insert Comparisons Here 4.2 Random Forest Model 4.2.1 Methodology and Analysis The Random Forest model was tuned similarly to the xgboost model using cross-validation. This model performed worse than the xgboost model on the training set, but more in line with model scores on the validation datasets. The AROC for the random forest .75 and the misclassification rate was -. The area under the ROC curve and Confusion matrix are shown below. Predicted Team 2 Won Predicted Team 1 Won Actual Team 2 Won 246 109 Actual Team 1 Won 118 255 Classification Rate 68.82% The random forest model also displays the most important variables in determining the winning team. A table of the most important variables to the model based on change in accuracy and impurity are shown below. This table also shows the Pomeroy rankings for both teams are the most influential in determining the winner. 4.2.2 Results The random forest model had similar performance on the 2019 NCAA tournament compared to the training. The model predicted 73% of the games correctly during this tournament with an AROC of .7963. The model performed worse on the 2021 tournament with a classification rate of 65% and an AROC of .714. The table below shows the model performance on the two tournaments  4.3 Neural Network 4.3.1 Methodology and Analysis In order to model the training set with a neural network, I standardized the data using z-score transformations. This process allows the predictor variables to be on the same scale so that continuous variables with large values or standard deviations do not dominate the model. The neural network also needed to be optimized for the number of hidden layers and decay (a regularization parameter to prevent overfitting).The final neural net model had an AROC of .845 and a classification rate of  on the training dataset. Predicted Team 2 Won Predicted Team 1 Won Actual Team 2 Won 270 89 Actual Team 1 Won 94 275 Classification Rate 74.86% 4.3.2 Results The Neural Network model performed relatively averagely on 2019 tournament, but better than either of the two other models on the 2021 tournament. The model predicted only 70% of the games correctly in 2019 with a AROC of .7848. However, the model predicted 68% of the 2021 tournament games correctly with an AROC of .7241. 4.4 Ensemble Models Looking at all of the machine learning models, each had strengths and weaknesses in predicting certain aspect of game scenarios. Combining the insights from these models may lead to even better predictions. 4.4.1 Random Forest and Neural Network "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
