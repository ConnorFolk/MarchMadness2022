[["index.html", "March Madness Machine Learning 2022 Chapter 1 Overview", " March Madness Machine Learning 2022 Connor Folk 2022-03-14 Chapter 1 Overview March is here, and everyone is talking about the NCAA tournament. Being a UNC fan, the talk this year is whether or not we will make the tournament. After watching ESPN talk about the seeding and teams, I wondered if I could use my data science knowledge to predict the outcome of these games. I am not talking about a perfect bracket, but one that used team stats to predict games. Could this model beat other peoples brackets and maybe win me a bit of money. As I initially searched the internet for NCAA basketball data, I stumbled upon a Kaggle competition that happens each year. This competition contained all the data I needed (and much more) to train some machine learning models. If you want to check out the site of this competition, it is listed here: https://www.kaggle.com/c/mens-march-mania-2022/ "],["dataset.html", "Chapter 2 Dataset", " Chapter 2 Dataset The data contains 20 CSV files with team names, matchups, box score stats, and rankings dating back to 2003. I started by looking at the box scores of each team since these statistics would be the basis of my models. Each row had a Winning Team and Losing Team along with their respective stats. I pivoted this dataset longer in order to have a row for each team in a game (one game has two associated rows, one for each team). These rows contained offensive statistics for the team like points and defensive statistics like points allowed. Then, I performed feature creation on the game stats: adding offensive rebounds and defensive rebounds together to calculate total rebound, subtracting points scored and allowed for point differential, and calculating possessions from shot attempts. After creating these columns, I rolled the game data into season data for each team by calculating the averages of each statistic over a season. I also created a few other features, including Points per Possession and effective Field Goal Percentage. After calculating the season statistics for each team, I joined the dataset with the ranking CSV. This file had rankings for each team, including the AP Poll, NET ranking, and Pomeroy ranking. I also created a sort of heat index, which showed the change in a teams Pom ranking over the last month before the tournament. I joined this table with seasonal and ranking data for each team with the file of team matchups for the 2003 to 2021 tournaments. Each row, now, contained Team 1 and Team 2 and their respective statistics for the year. The results were set up as a binary indicator coinciding with a 1 if Team 1 won and a 0 if Team 2 won. This means the probability of Team 1 winning is just p, while the probability of Team 2 winning is 1-p.Â The dataset is also evenly split between games that Team 1 and Team 2 won. Lastly, I filtered the training data to only include data from the 2008 to 2018 seasons. This ensured that the model had enough data points (around 600) to find patterns but did not look back too far at seasons in which the college basketball landscape was vastly different. My validation and test sets were the 2019 and 2021 seasons and tournaments (NOTE: There was no tournament in 2020 due to COVID-19) "],["variable-selection.html", "Chapter 3 Variable Selection", " Chapter 3 Variable Selection The dataset contained 112 different variables or metrics for each team. Using all of these variables provided the modeling techniques with too much noise. Many modeling techniques predicted the training data well with all of the variables but performed poorly on the test and validation. Using a combination of variable importance plots, modeling assessment metrics, and knowledge of college basketball, I choose the following variables: Points Per Possession (Offensive Efficiency)- Average number of points a team scores per possession Opponents Points Per Possession (Defensive Efficiency)- Average number of points allowed per possession Point Differential- Margin of victory, number of points scored minus number of points allowed Pomeroy Ranking- Ranking of teams by legendary college basketball statistician Ken Pom (lower the ranking, the better) Opponent Three Point Field Goal Percentage- Number of three points allowed divided by number of three points attempted by opponent Free Throw Percentage- Number of free throws made/ number of free throws attempted Offensive Rebound Difference- Difference between a teams number of offensive rebounds and their opponents number of offensive rebounds Opponent Turnovers- Number of turnovers by a teams opponent These variables account for a teams defensive, offensive, rebounding, and overall abilities. "],["models.html", "Chapter 4 Models 4.1 XGBoost 4.2 Random Forest Model 4.3 Neural Network 4.4 Ensemble Models", " Chapter 4 Models After data cleaning and variable selection, I created different models, including logistic regression, XGBoost, random forest, and neural networks, and scored them based on the area under the ROC curve (AROC) and misclassification rate. AROC showed the models performance across different cutoffs, which may not seem useful since the cutoff should be .5. However, this metric allowed me to assess models overall performance, not just classification. The three best models were an XGBoost model, a random forest model, and a neural net model. 4.1 XGBoost 4.1.1 Methodology and Analysis In order to create this model, I had to tune the parameters to the dataset. I used 30-fold cross validation to pick the parameters that resulted in the lowest amount of error and prevented overfitting. However, this model still overfits the training data. The ROC curve and Confusion Matrix for this model on the training data is shown below. The model does extraordinarily well with an AROC of .94 and a classification rate of 88.32%. Table 4.1: XGBoost Confusion Matrix on Training Data Actual Team 2 Won Actual Team 1 Won Predicted Team 2 Won 322 43 Predicted Team 1 Won 42 321 Classification Rate 88.32% The model also calculates the importance of each variable in predicting which team wins. The variables are clustered based on importance to the model, with cluster 1 in red and cluster 2 in light blue. The two most important variables were the Pomeroy rankings for the two teams. The plot below shows each variables importance in the model. 4.1.2 Results Even though the model performed extremely well on the training set, it does not perform as well on the validation dataset (2019 and 2021 seasons) compared to the training dataset furthering the evidence that the model is overfitted. However, the model performs very well on the 2019 NCAA tournament compared to other models I created, correctly predicting about 76% of the games. However, the model performs poorly on the 2021 NCAA tournament, predicting only 63% of games correctly. The table below contains accuracy measures for the XGBoost model for the 2019 and 2021 tournaments. Table 4.2: XGBoost Measures on Validation Data Classification Rate AROC 2019 Tournament 76.12% 80.17% 2021 Tournament 63.64% 66.67% 4.2 Random Forest Model 4.2.1 Methodology and Analysis The Random Forest model was tuned similarly to the XGBoost model using cross-validation. This model performed worse than the XGBoost model on the training set but was more aligned with model scores on the validation datasets. The AROC for the random forest was .75, and the misclassification rate was 68.82%. Below are the area under the ROC curve and Confusion matrix. Table 4.3: Random Forest Confusion Matrix on Training Data Actual Team 2 Won Actual Team 1 Won Predicted Team 2 Won 246 109 Predicted Team 1 Won 118 255 Classification Rate 68.82% The random forest model also displays the most important variables in determining the winning team. A table of the most important variables to the model based on change in accuracy and impurity is shown below. This table also indicates the Pomeroy rankings for both teams are the most influential in determining the winner. 4.2.2 Results The random forest model had a similar performance on the 2019 NCAA tournament compared to the training. The model predicted 73% of the games correctly during this tournament with an AROC of .7963. The model performed worse on the 2021 tournament, with a classification rate of 63% and an AROC of .714. The table below shows the model performance on the two tournaments is shown in the table below. Table 4.4: Random Forest Measures on Validation Data Classification Rate AROC 2019 Tournament 73.13% 79.63% 2021 Tournament 63.64% 71.40% 4.3 Neural Network 4.3.1 Methodology and Analysis To model the training set with a neural network, I standardized the data using z-score transformations. This process allows the predictor variables to be on the same scale so that continuous variables with large values or standard deviations do not dominate the model. The neural network also needed to be optimized for the number of hidden layers and decay (a regularization parameter to prevent overfitting). The final neural net model had an AROC of .845 and a classification rate of 74.86% on the training dataset. Table 4.5: Neural Network Confusion Matrix on Training Data Actual Team 2 Won Actual Team 1 Won Predicted Team 2 Won 270 89 Predicted Team 1 Won 94 275 Classification Rate 74.86% 4.3.2 Results The Neural Network model performed relatively averagely on the 2019 tournament but better than the two other models on the 2021 tournament. The model predicted only 70% of the games correctly in 2019 with an AROC of .7848. However, the model predicted 68% of the 2021 tournament games correctly with an AROC of .7241. A table with the accuracy measures for the Neural Network model is shown below. Table 4.6: Neural Network Measures on Validation Data Classification Rate AROC 2019 Tournament 70.15% 78.48% 2021 Tournament 68.18% 72.41% 4.4 Ensemble Models Looking at all of the machine learning models, each had strengths and weaknesses in predicting certain aspects of game scenarios. Combining the insights from these models may lead to even better predictions. 4.4.1 Random Forest and Neural Network Below are the Confusion Matrices for Neural Network and Random Forest models on the 2019 NCAA tournament. Both models performed relatively well on this tournament, with the NN classifying 70.15% of games correctly and the Random Forest classifying 73.13% of games correctly. However, the models seem to perform better in different classifying sections. The Neural Network was extremely good at predicting whether Team 1 Won, correctly classifying 85% of predicted Team 1 Wins. However, the model was worse at predicting Team 2 wins, only correctly classifying 64% of predicted Team 2 wins. The Random Forest model was more balanced, correctly classifying 74% of predicted Team 2 wins and 72% of predicted Team 1 wins. Combining the predictions from these two models may actually improve performance on new data. Table 4.7: Neural Network Confusion Matrix on 2019 NCAA Tournament Actual Team 2 Won Actual Team 1 Won Predicted Team 2 Won 30 17 Predicted Team 1 Won 3 17 Classification Rate 70.15% Table 4.7: Random Forest Confusion Matrix on 2019 NCAA Tournament Actual Team 2 Won Actual Team 1 Won Predicted Team 2 Won 23 8 Predicted Team 1 Won 10 26 Classification Rate 73.13% When combining the predictions from both Models, model performance on the 2019 tournament is consistent but is actually better on the 2021 tournament. The RF/NN Ensemble confusion matrices for the 2019 and 2021 tournaments are shown below. This model predicts almost 70% of games correctly in the 2021 tournament compared to 68% by the Neural Network model alone and 64% by the Random Forest model alone. Table 4.8: Random Forest and Neural Net Ensemble Confusion Matrix on 2019 NCAA Tournament Actual Team 2 Won Actual Team 1 Won Predicted Team 2 Won 30 15 Predicted Team 1 Won 3 19 Classification Rate 73.13% Table 4.8: Random Forest and Neural Net Ensemble Confusion Matrix on 2021 NCAA Tournament Actual Team 2 Won Actual Team 1 Won Predicted Team 2 Won 25 12 Predicted Team 1 Won 8 21 Classification Rate 69.7% 4.4.2 Random Forest, Neural Network, and XGBoost Adding the predictions from the XGBoost model may increase performance even further. The XGBoost overfits the training data and, therefore, does not perform consistently on the validation sets. However, combining these predictions to the NN/RF Ensemble model may make the predictions more generalizable while extracting as much signal from the data as possible. The confusion matrices for the NN/RF/XG Ensemble model for the 2019 and 2021 NCAA tournaments are in the table below. This model performs the best on both the 2019 and 2021 datasets, even breaking into the 70% accuracy range for the 2021 tournament. Table 4.9: Random Forest, Neural Net, and XGBoost Ensemble Confusion Matrix on 2019 NCAA Tournament Actual Team 2 Won Actual Team 1 Won Predicted Team 2 Won 29 12 Predicted Team 1 Won 4 22 Classification Rate 76.12% Table 4.9: Random Forest, Neural Net, and XGBoost Ensemble Confusion Matrix on 2021 NCAA Tournament Actual Team 2 Won Actual Team 1 Won Predicted Team 2 Won 25 11 Predicted Team 1 Won 8 22 Classification Rate 71.21% "],["assessing-model-performance.html", "Chapter 5 Assessing Model Performance", " Chapter 5 Assessing Model Performance The classification rates of the models are hard to interpret alone. For example, is predicting 76% of the 2021 tournament games impressive or just better than the other models? In order to assess model performance, I decided to use the selection committees ranking as a baseline, comparing the models accuracy to the accuracy of just picking the higher seeded team (known as a Chalk Bracket). The classification rates for the Chalk method and NN/RF/XG Ensemble method are in the table below. Just picking the higher seed was actually pretty effective for the 2019 tournament, resulting in an accuracy of 72%. However, the ensemble model still performed 6% better than the Chalk method in 2019. The Chalk method was much less accurate in picking the 2021 tournament games, coinciding with only a 61% classification rate. The ensemble model far outperformed the Chalk method during the 2021 tournament with a 17% higher accuracy score. Table 5.1: NN/RF/XG Assessment Chalk Accuracy Model Accuracy Percent Difference 2019 Tournament 71.64% 76.12% 6.06% 2021 Tournament 60.61% 71.21% 16.85% The ensemble model performed better than picking higher seeds in two very different tournament scenarios with an accuracy above 70%. Combining all three predictions increased the generalizability of the model while preserving accurate predictions. "],["ncaa-tournament.html", "Chapter 6 2022 NCAA Tournament 6.1 Data Preparation 6.2 Model Rankings", " Chapter 6 2022 NCAA Tournament 6.1 Data Preparation When looking at predictions of the model, they seemed to vary depending on the labeling of the teams as Team 1 and Team 2. In order to decrease this variability, I added an additional row per game with the opposite labeling of Team 1 and Team 2. The model can know view the same game result with Team 1 winning and Team 2 winning, hopefully decreasing the variability in the randomness of labeling the teams. Once this data change was made, the models Log Loss for the 2021 tournament decreased to about .59, which would have placed top 50 in that competition. 6.2 Model Rankings Once running the model with the new data for the 2021-2022 season, predictions could be made for the 2022 tournament. The ranking of the teams based on average probability of winning against any team in the tournament is below. Higher probabilities mean that team is likely to win in more matchups in the tournament. Table 6.1: 2022 Rankings Rank Team AVGPrediction 1 Gonzaga 0.7688933 2 Arizona 0.7685462 3 Houston 0.7241145 4 Iowa 0.7150696 5 Baylor 0.6945261 6 Kentucky 0.6921316 7 UCLA 0.6896294 8 Kansas 0.6596790 9 Duke 0.6566141 10 Texas Tech 0.6532578 11 Auburn 0.6514166 12 Villanova 0.6399042 13 Tennessee 0.6322477 14 Texas 0.6268857 15 Murray St 0.6261029 16 LSU 0.6167269 17 Purdue 0.6156445 18 Connecticut 0.6053684 19 Arkansas 0.5964973 20 St Marys CA 0.5952203 21 Illinois 0.5890800 22 San Francisco 0.5853186 23 Loyola-Chicago 0.5681378 24 UAB 0.5487578 25 Memphis 0.5398739 26 San Diego St 0.5382957 27 North Carolina 0.5347638 28 Boise St 0.5295159 29 Virginia Tech 0.5177411 30 USC 0.5105664 31 Iowa St 0.5013822 32 Vermont 0.4999403 33 Alabama 0.4975716 34 Ohio St 0.4969482 35 Colorado St 0.4964146 36 Seton Hall 0.4948429 37 Wisconsin 0.4867273 38 Marquette 0.4831867 39 Michigan 0.4830376 40 Michigan St 0.4755332 41 TCU 0.4648768 42 Providence 0.4629303 43 Davidson 0.4590377 44 Notre Dame 0.4573141 45 Chattanooga 0.4444356 46 S Dakota St 0.4393861 47 Wyoming 0.4391584 48 Miami FL 0.4157946 49 Indiana 0.4087451 50 Rutgers 0.4048843 51 New Mexico St 0.3914793 52 Creighton 0.3880162 53 Richmond 0.3721586 54 Georgia St 0.3687587 55 St Peters 0.3583957 56 Yale 0.3570951 57 CS Fullerton 0.3468519 58 Longwood 0.3468082 59 Montana St 0.3295265 60 Delaware 0.3234417 61 Jacksonville St 0.3205380 62 Colgate 0.3154666 63 Norfolk St 0.3126803 64 Wright St 0.3063850 65 TX Southern 0.2941161 66 Bryant 0.2921967 67 Akron 0.2888322 68 TAM C. Christi 0.2845785 "]]
